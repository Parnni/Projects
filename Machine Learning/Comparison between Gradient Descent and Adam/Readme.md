# Project - Comparison between Gradient Descent and Adam.

## Table of Contents
- [Overview](#Overview)
- [Technologies Used](#Technologies-Used)
- [Result](#Result)
- [Credits](#Credits)

## Overview
In this project, I have worked on optimizing the Gradient Descent algorithm. I have programmed an algorithm named 'Adam' from scratch and both the algorithms are then compared for better performance.

The program includes different methods such as 
- Weight initialization.
- Regularization.
- Dropout.
- Optimizers.

## Technologies Used
Jupyter Notebook

## Result
Under same conditions<br>
1. Adam reduces the cost faster than Gradient descent.
2. Accuracy of Adam is higher than Gradient descent.

<img src = "https://github.com/Parnni/Projects/blob/main/Machine%20Learning/0_Images/GDvsAdam.PNG" width = "500" height="400"/>

## Credits
- I have learned the DNN from Andrew Ng's Machine Learning Course on Coursera.
- The data used is from Andrew Ng's Machine Learning Course.
